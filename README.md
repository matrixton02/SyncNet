# ğŸš€ SyncNet: Distributed ML Training Across Laptops  

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)  
![PyTorch](https://img.shields.io/badge/PyTorch-ML-orange.svg)  
![Hackathon](https://img.shields.io/badge/Hackathon-Top%2015%20Project-success.svg)  

## ğŸ“Œ Overview  
**SyncNet** is a lightweight framework for **distributed machine learning training** that allows multiple laptops on the same network to collaboratively train a single ML model â€” without requiring expensive GPUs or cloud services.  

- One device runs as the **server** (coordinator).  
- Other devices connect as **clients**.  
- The dataset and model are **split across clients** â†’ each client trains locally â†’ weights are aggregated (FedAvg / FedProx).  
- A **real-time dashboard** shows training progress for each client.  

This project was built during a hackathon, where it placed **Top 15 out of 296 teams** ğŸ†.  

---

## ğŸ¯ Problem Statement  
Training ML models in hackathons or student projects is often bottlenecked by **limited compute**.  
- Teams cannot finish training large models within time.  
- Cloud GPUs are expensive and difficult to set up quickly.  
- Existing federated learning frameworks are heavy and impractical for small setups.  

**SyncNet solves this** by enabling fast, collaborative training across multiple laptops connected via LAN/WiFi.  

---

## âš¡ Key Features  
âœ… **Custom Models** â€“ Supports flexible MLPs (user-defined layers) and CNNs.  
âœ… **Training Algorithms** â€“ Implements both **FedAvg** and **FedProx** (adaptive Î¼).  
âœ… **IID & Non-IID Splits** â€“ Can simulate real-world skewed data distributions.  
âœ… **Parallel Training** â€“ Clients train simultaneously, reducing wall-clock time.  
âœ… **Progress Dashboard** â€“ Real-time monitoring of client training (loss, accuracy, progress).  
âœ… **Custom Dataset Support** â€“ Use MNIST or upload your own CSV dataset.  

---

## ğŸ› ï¸ Technical Approach  
- **Model Distribution:** Model + dataset chunks sent via socket protocol (`send_msg`, `recv_msg`).  
- **Dynamic Architectures:**  
  - `FlexibleNN` â†’ arbitrary hidden layers for MLPs.  
  - `SimpleCNN` â†’ lightweight CNN for image datasets.  
- **Federated Training:**  
  - FedAvg for IID data.  
  - FedProx (with adaptive Î¼ and gradient clipping) for Non-IID data to reduce client drift.  
- **Aggregation:** Server collects weights, averages them, and produces the final global model.  

---

## ğŸ“‚ Project Structure  
```
SyncNet/
â”‚â”€â”€ server.py # Server (coordinator) code
â”‚â”€â”€ client.py # Client (worker) code
â”‚â”€â”€ main.py # Entry point (choose server/client mode)
â”‚â”€â”€ model.py # Model definitions (MLP, CNN)
â”‚â”€â”€ utils.py # Helper functions (send/recv messages, serialization, etc.)
â”‚â”€â”€ dashboard.py # Flask-based real-time dashboard
â”‚â”€â”€ test_model.py # Test bench (loads saved model and evaluates)
â”‚â”€â”€ README.md # Project documentation
```
---

## ğŸš€ Getting Started  

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/<your-username>/SyncNet.git
cd SyncNet
```
## 2ï¸âƒ£ Install Requirements
```bash
pip install torch torchvision flask pandas numpy
```
## 3ï¸âƒ£ Run Server
```bash
python main.py
# Choose option 1: Run as Server
```
## 4ï¸âƒ£ Run Clients
```bash
python main.py
# Choose option 2: Run as Client
# get the ip address of the server and enter the port number
# make sure all clients trying to access the server are connected on the same network
#for testing select 2 layered mlp 128 and 64 neurons then chhose iid and minst data set
```
## 5ï¸âƒ£ Testing the Model
```bash
python test_model.py
```
## ğŸ“Š Results

Centralized (single machine, 20 epochs, 2-layer MLP): ~97% accuracy (60s).

Distributed (3 clients, FedAvg, 25 epochs): ~95% accuracy (12s).

Non-IID (3 clients, FedProx, 5 epochs, 128â€“64 MLP): ~85â€“90% accuracy.

CNN (5 epochs, distributed): ~93â€“95% accuracy.

## ğŸ’¡ Future Plans

 Support more algorithms (FedNova, Scaffold, FedOpt).

 Work on the efficiency of fedprox.

 Add CIFAR-10 and larger datasets.

 Implement P2P communication (no single server).

 Provide Docker setup for easy deployment.

 Integrate with cloud + edge devices (hybrid training).

## ğŸ‘¥ Team

Built during a hackathon by Team SyncNet ğŸš€.
